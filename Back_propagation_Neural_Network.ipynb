{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Back_propagation_Neural_Network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saiful-Bahri-Irfanto/Machine-Learning/blob/master/Back_propagation_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmkt1-laEYyO",
        "colab_type": "text"
      },
      "source": [
        "sumber **(https://www.kaggle.com/andreicosma/back-propagation-neural-network)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRbQmY6V2kUv",
        "colab_type": "text"
      },
      "source": [
        "Import library sng dibutuhno"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-euS4MQf2pV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcOf62gF2wRp",
        "colab_type": "text"
      },
      "source": [
        "Load data dari google drive, dataset nya simpan dulu ke google drive terus di panggil nanti"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "solyB_ggc-9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d6c20a3-285d-4b78-897d-7c8dfaf8b9d4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faYDCPGN_Y38",
        "colab_type": "text"
      },
      "source": [
        "Pastikan direktori datset nya sama contoh aku simpan datasetnya di drive google aku buat folder dengan nama analisis data, terus di dalam anaisis data ada folder dataset, nah di dalam folder dataset tersebut baru ada dataset yang digunakan untuk project ini"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIg_vMgu4PJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "5e801a9d-5d5e-4fc5-df31-5519aafb56f2"
      },
      "source": [
        "db = pd.read_csv(\"/content/drive/My Drive/analis data/dataset/duke-breast-cancer.txt\", sep = \"\\t\")\n",
        "db.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>-0.362772</th>\n",
              "      <th>-0.314085</th>\n",
              "      <th>-0.177185</th>\n",
              "      <th>0.89865</th>\n",
              "      <th>-0.154588</th>\n",
              "      <th>-0.217807</th>\n",
              "      <th>0.410032</th>\n",
              "      <th>-0.106035</th>\n",
              "      <th>2.09458</th>\n",
              "      <th>-0.120402</th>\n",
              "      <th>-0.227627</th>\n",
              "      <th>-0.611535</th>\n",
              "      <th>0.456636</th>\n",
              "      <th>0.409078</th>\n",
              "      <th>-0.275131</th>\n",
              "      <th>-0.194476</th>\n",
              "      <th>0.083471</th>\n",
              "      <th>-0.399868</th>\n",
              "      <th>-0.299734</th>\n",
              "      <th>-0.499663</th>\n",
              "      <th>-0.449116</th>\n",
              "      <th>0.014423</th>\n",
              "      <th>-0.263376</th>\n",
              "      <th>-0.228857</th>\n",
              "      <th>-0.365664</th>\n",
              "      <th>0.235682</th>\n",
              "      <th>-0.858571</th>\n",
              "      <th>-0.371278</th>\n",
              "      <th>-0.489651</th>\n",
              "      <th>1.43927</th>\n",
              "      <th>-0.207108</th>\n",
              "      <th>-0.241611</th>\n",
              "      <th>0.319242</th>\n",
              "      <th>-0.534551</th>\n",
              "      <th>-0.497377</th>\n",
              "      <th>-0.056419</th>\n",
              "      <th>-0.792793</th>\n",
              "      <th>0.008452</th>\n",
              "      <th>-0.722719</th>\n",
              "      <th>...</th>\n",
              "      <th>-0.222273</th>\n",
              "      <th>0.079099</th>\n",
              "      <th>0.12378</th>\n",
              "      <th>-0.250833</th>\n",
              "      <th>0.808906</th>\n",
              "      <th>0.264752</th>\n",
              "      <th>-0.641295</th>\n",
              "      <th>-0.146375</th>\n",
              "      <th>0.250552</th>\n",
              "      <th>-1.15369</th>\n",
              "      <th>0.109339</th>\n",
              "      <th>-0.21984</th>\n",
              "      <th>0.135802</th>\n",
              "      <th>1.16252</th>\n",
              "      <th>-0.506647</th>\n",
              "      <th>-0.21552</th>\n",
              "      <th>-0.161985</th>\n",
              "      <th>-0.270079</th>\n",
              "      <th>-0.038109</th>\n",
              "      <th>0.545737</th>\n",
              "      <th>-0.030848</th>\n",
              "      <th>-0.131192</th>\n",
              "      <th>0.37993</th>\n",
              "      <th>0.067359</th>\n",
              "      <th>0.347915</th>\n",
              "      <th>-0.05617</th>\n",
              "      <th>-0.260292</th>\n",
              "      <th>0.3255</th>\n",
              "      <th>-0.153237</th>\n",
              "      <th>0.560396</th>\n",
              "      <th>0.352048</th>\n",
              "      <th>-0.163001</th>\n",
              "      <th>0.050292</th>\n",
              "      <th>0.471149</th>\n",
              "      <th>0.241524</th>\n",
              "      <th>0.068197</th>\n",
              "      <th>-0.009899</th>\n",
              "      <th>0.343084</th>\n",
              "      <th>-1.24348</th>\n",
              "      <th>1.80402</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>-0.459580</td>\n",
              "      <td>-0.719855</td>\n",
              "      <td>-1.005840</td>\n",
              "      <td>-0.902675</td>\n",
              "      <td>-0.657600</td>\n",
              "      <td>-0.592310</td>\n",
              "      <td>0.233404</td>\n",
              "      <td>-0.745314</td>\n",
              "      <td>-0.736191</td>\n",
              "      <td>-0.722748</td>\n",
              "      <td>-0.532631</td>\n",
              "      <td>-0.019970</td>\n",
              "      <td>-0.027846</td>\n",
              "      <td>-0.856911</td>\n",
              "      <td>1.377600</td>\n",
              "      <td>-0.036926</td>\n",
              "      <td>-0.245902</td>\n",
              "      <td>-0.289624</td>\n",
              "      <td>-0.524521</td>\n",
              "      <td>-0.696216</td>\n",
              "      <td>-0.589519</td>\n",
              "      <td>0.169156</td>\n",
              "      <td>1.069250</td>\n",
              "      <td>-0.260436</td>\n",
              "      <td>-0.538246</td>\n",
              "      <td>-0.539099</td>\n",
              "      <td>0.469226</td>\n",
              "      <td>0.195402</td>\n",
              "      <td>0.518308</td>\n",
              "      <td>-0.392350</td>\n",
              "      <td>-0.167847</td>\n",
              "      <td>-0.707957</td>\n",
              "      <td>-0.835457</td>\n",
              "      <td>0.362133</td>\n",
              "      <td>-0.444962</td>\n",
              "      <td>-0.126178</td>\n",
              "      <td>0.347473</td>\n",
              "      <td>1.777980</td>\n",
              "      <td>-0.778002</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.488066</td>\n",
              "      <td>-0.166436</td>\n",
              "      <td>-0.304630</td>\n",
              "      <td>-0.538749</td>\n",
              "      <td>-1.481340</td>\n",
              "      <td>-0.049541</td>\n",
              "      <td>-0.174811</td>\n",
              "      <td>-0.255201</td>\n",
              "      <td>-0.551215</td>\n",
              "      <td>0.284865</td>\n",
              "      <td>-0.101783</td>\n",
              "      <td>-0.415393</td>\n",
              "      <td>-0.654582</td>\n",
              "      <td>5.769090</td>\n",
              "      <td>-0.355934</td>\n",
              "      <td>-0.740597</td>\n",
              "      <td>-0.542211</td>\n",
              "      <td>-0.552544</td>\n",
              "      <td>-0.067350</td>\n",
              "      <td>0.484912</td>\n",
              "      <td>-0.678985</td>\n",
              "      <td>-0.614250</td>\n",
              "      <td>-0.098147</td>\n",
              "      <td>-0.717096</td>\n",
              "      <td>-0.375175</td>\n",
              "      <td>-0.679817</td>\n",
              "      <td>-0.397442</td>\n",
              "      <td>-1.158800</td>\n",
              "      <td>-0.598238</td>\n",
              "      <td>0.070776</td>\n",
              "      <td>0.224620</td>\n",
              "      <td>-0.003717</td>\n",
              "      <td>-0.405143</td>\n",
              "      <td>0.251173</td>\n",
              "      <td>0.368900</td>\n",
              "      <td>-0.312117</td>\n",
              "      <td>-0.573882</td>\n",
              "      <td>-0.729734</td>\n",
              "      <td>-0.639368</td>\n",
              "      <td>0.344724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.103909</td>\n",
              "      <td>-0.296076</td>\n",
              "      <td>-0.165474</td>\n",
              "      <td>0.044957</td>\n",
              "      <td>-0.089225</td>\n",
              "      <td>-0.380194</td>\n",
              "      <td>0.249268</td>\n",
              "      <td>-0.219473</td>\n",
              "      <td>-0.343635</td>\n",
              "      <td>0.600622</td>\n",
              "      <td>-0.080622</td>\n",
              "      <td>0.181226</td>\n",
              "      <td>0.162455</td>\n",
              "      <td>-0.511097</td>\n",
              "      <td>1.241950</td>\n",
              "      <td>-0.013042</td>\n",
              "      <td>-0.083209</td>\n",
              "      <td>-0.362484</td>\n",
              "      <td>-0.028900</td>\n",
              "      <td>-0.457592</td>\n",
              "      <td>-0.484332</td>\n",
              "      <td>-0.296829</td>\n",
              "      <td>0.107685</td>\n",
              "      <td>0.037133</td>\n",
              "      <td>-0.127737</td>\n",
              "      <td>-0.049854</td>\n",
              "      <td>-0.593527</td>\n",
              "      <td>-0.878698</td>\n",
              "      <td>0.435926</td>\n",
              "      <td>0.245502</td>\n",
              "      <td>-0.625318</td>\n",
              "      <td>-0.117213</td>\n",
              "      <td>0.621393</td>\n",
              "      <td>-0.794839</td>\n",
              "      <td>-0.831591</td>\n",
              "      <td>0.838897</td>\n",
              "      <td>-0.228268</td>\n",
              "      <td>0.075216</td>\n",
              "      <td>-0.081244</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.134623</td>\n",
              "      <td>-0.472963</td>\n",
              "      <td>0.260643</td>\n",
              "      <td>-0.086674</td>\n",
              "      <td>0.370911</td>\n",
              "      <td>0.043302</td>\n",
              "      <td>0.999930</td>\n",
              "      <td>-0.327108</td>\n",
              "      <td>0.169201</td>\n",
              "      <td>0.123854</td>\n",
              "      <td>-0.119601</td>\n",
              "      <td>-0.136502</td>\n",
              "      <td>-0.085859</td>\n",
              "      <td>0.497591</td>\n",
              "      <td>-0.340138</td>\n",
              "      <td>0.120833</td>\n",
              "      <td>0.075918</td>\n",
              "      <td>0.370708</td>\n",
              "      <td>0.164324</td>\n",
              "      <td>0.490828</td>\n",
              "      <td>0.017628</td>\n",
              "      <td>0.035919</td>\n",
              "      <td>0.026320</td>\n",
              "      <td>0.171399</td>\n",
              "      <td>-0.195496</td>\n",
              "      <td>0.025022</td>\n",
              "      <td>0.130956</td>\n",
              "      <td>0.713825</td>\n",
              "      <td>-0.066845</td>\n",
              "      <td>0.179875</td>\n",
              "      <td>0.388621</td>\n",
              "      <td>-0.488521</td>\n",
              "      <td>-0.405670</td>\n",
              "      <td>0.346316</td>\n",
              "      <td>-0.044239</td>\n",
              "      <td>0.095011</td>\n",
              "      <td>0.430106</td>\n",
              "      <td>0.181200</td>\n",
              "      <td>-0.626812</td>\n",
              "      <td>0.237308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.110728</td>\n",
              "      <td>-0.147049</td>\n",
              "      <td>-0.402544</td>\n",
              "      <td>-0.148652</td>\n",
              "      <td>-0.112891</td>\n",
              "      <td>-0.047957</td>\n",
              "      <td>0.132846</td>\n",
              "      <td>0.064040</td>\n",
              "      <td>-0.319076</td>\n",
              "      <td>0.923248</td>\n",
              "      <td>-0.011552</td>\n",
              "      <td>-0.263451</td>\n",
              "      <td>-0.350887</td>\n",
              "      <td>-0.679302</td>\n",
              "      <td>0.457169</td>\n",
              "      <td>-0.620489</td>\n",
              "      <td>0.499716</td>\n",
              "      <td>-0.324323</td>\n",
              "      <td>-0.167195</td>\n",
              "      <td>-0.595894</td>\n",
              "      <td>-0.202526</td>\n",
              "      <td>-0.043874</td>\n",
              "      <td>0.164872</td>\n",
              "      <td>-0.262141</td>\n",
              "      <td>-0.225273</td>\n",
              "      <td>-0.382358</td>\n",
              "      <td>1.466430</td>\n",
              "      <td>-0.397152</td>\n",
              "      <td>-0.240803</td>\n",
              "      <td>0.030311</td>\n",
              "      <td>-0.595990</td>\n",
              "      <td>-0.888365</td>\n",
              "      <td>0.322103</td>\n",
              "      <td>0.756694</td>\n",
              "      <td>-1.006570</td>\n",
              "      <td>-0.593973</td>\n",
              "      <td>-0.564075</td>\n",
              "      <td>-0.995814</td>\n",
              "      <td>-0.435999</td>\n",
              "      <td>...</td>\n",
              "      <td>0.232984</td>\n",
              "      <td>-0.595563</td>\n",
              "      <td>0.685015</td>\n",
              "      <td>0.324595</td>\n",
              "      <td>-0.382599</td>\n",
              "      <td>-0.413532</td>\n",
              "      <td>0.400038</td>\n",
              "      <td>0.301978</td>\n",
              "      <td>-0.984536</td>\n",
              "      <td>0.586596</td>\n",
              "      <td>0.362638</td>\n",
              "      <td>-0.033752</td>\n",
              "      <td>0.082420</td>\n",
              "      <td>-0.058723</td>\n",
              "      <td>-0.272900</td>\n",
              "      <td>0.051017</td>\n",
              "      <td>0.203256</td>\n",
              "      <td>-0.081056</td>\n",
              "      <td>0.159498</td>\n",
              "      <td>0.315275</td>\n",
              "      <td>0.106651</td>\n",
              "      <td>-0.183165</td>\n",
              "      <td>-0.422182</td>\n",
              "      <td>0.582537</td>\n",
              "      <td>0.591026</td>\n",
              "      <td>0.280486</td>\n",
              "      <td>0.532511</td>\n",
              "      <td>-0.446665</td>\n",
              "      <td>-0.602023</td>\n",
              "      <td>-0.235652</td>\n",
              "      <td>0.419011</td>\n",
              "      <td>-0.362254</td>\n",
              "      <td>-0.077094</td>\n",
              "      <td>0.517718</td>\n",
              "      <td>-0.591872</td>\n",
              "      <td>0.063579</td>\n",
              "      <td>-0.094244</td>\n",
              "      <td>0.731982</td>\n",
              "      <td>-1.464370</td>\n",
              "      <td>0.905178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.332963</td>\n",
              "      <td>1.002860</td>\n",
              "      <td>0.803334</td>\n",
              "      <td>-0.431752</td>\n",
              "      <td>1.479780</td>\n",
              "      <td>0.130415</td>\n",
              "      <td>0.323632</td>\n",
              "      <td>0.472668</td>\n",
              "      <td>0.237987</td>\n",
              "      <td>0.706464</td>\n",
              "      <td>0.490895</td>\n",
              "      <td>0.410346</td>\n",
              "      <td>-0.031808</td>\n",
              "      <td>-1.110020</td>\n",
              "      <td>-0.616352</td>\n",
              "      <td>-0.252793</td>\n",
              "      <td>0.393914</td>\n",
              "      <td>0.286964</td>\n",
              "      <td>0.922988</td>\n",
              "      <td>0.099361</td>\n",
              "      <td>0.745967</td>\n",
              "      <td>-0.087043</td>\n",
              "      <td>0.556089</td>\n",
              "      <td>0.091012</td>\n",
              "      <td>-0.136108</td>\n",
              "      <td>-1.115780</td>\n",
              "      <td>-0.515446</td>\n",
              "      <td>-0.366578</td>\n",
              "      <td>0.519239</td>\n",
              "      <td>1.038630</td>\n",
              "      <td>-0.279246</td>\n",
              "      <td>-0.741718</td>\n",
              "      <td>1.137660</td>\n",
              "      <td>-1.616480</td>\n",
              "      <td>0.498838</td>\n",
              "      <td>-0.511916</td>\n",
              "      <td>0.350491</td>\n",
              "      <td>1.369830</td>\n",
              "      <td>1.127860</td>\n",
              "      <td>...</td>\n",
              "      <td>1.258130</td>\n",
              "      <td>0.317853</td>\n",
              "      <td>1.094880</td>\n",
              "      <td>0.459950</td>\n",
              "      <td>1.184390</td>\n",
              "      <td>0.936568</td>\n",
              "      <td>1.202350</td>\n",
              "      <td>0.919205</td>\n",
              "      <td>-0.023758</td>\n",
              "      <td>0.585960</td>\n",
              "      <td>0.521507</td>\n",
              "      <td>0.328081</td>\n",
              "      <td>1.350300</td>\n",
              "      <td>-0.144284</td>\n",
              "      <td>0.528373</td>\n",
              "      <td>0.349557</td>\n",
              "      <td>0.726116</td>\n",
              "      <td>1.233530</td>\n",
              "      <td>1.063070</td>\n",
              "      <td>-0.326382</td>\n",
              "      <td>0.853327</td>\n",
              "      <td>1.244250</td>\n",
              "      <td>0.576107</td>\n",
              "      <td>0.720982</td>\n",
              "      <td>1.785740</td>\n",
              "      <td>1.530860</td>\n",
              "      <td>0.167558</td>\n",
              "      <td>1.965730</td>\n",
              "      <td>-1.175750</td>\n",
              "      <td>-0.052883</td>\n",
              "      <td>1.117700</td>\n",
              "      <td>-0.099333</td>\n",
              "      <td>0.491889</td>\n",
              "      <td>2.226070</td>\n",
              "      <td>-0.415884</td>\n",
              "      <td>0.284093</td>\n",
              "      <td>1.911490</td>\n",
              "      <td>1.815050</td>\n",
              "      <td>-0.559660</td>\n",
              "      <td>-0.699785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>-0.237573</td>\n",
              "      <td>-0.429426</td>\n",
              "      <td>0.841753</td>\n",
              "      <td>-0.785126</td>\n",
              "      <td>0.463096</td>\n",
              "      <td>-0.133755</td>\n",
              "      <td>0.688543</td>\n",
              "      <td>-0.172954</td>\n",
              "      <td>-0.561986</td>\n",
              "      <td>-0.702004</td>\n",
              "      <td>-0.114303</td>\n",
              "      <td>-0.415897</td>\n",
              "      <td>0.183255</td>\n",
              "      <td>-0.354275</td>\n",
              "      <td>-0.407550</td>\n",
              "      <td>-0.457968</td>\n",
              "      <td>0.096522</td>\n",
              "      <td>-0.255143</td>\n",
              "      <td>-0.140487</td>\n",
              "      <td>-0.479721</td>\n",
              "      <td>-0.200128</td>\n",
              "      <td>0.422286</td>\n",
              "      <td>-0.412442</td>\n",
              "      <td>-0.267942</td>\n",
              "      <td>-0.000998</td>\n",
              "      <td>-0.267872</td>\n",
              "      <td>-0.614125</td>\n",
              "      <td>-0.439739</td>\n",
              "      <td>-0.359249</td>\n",
              "      <td>-0.057571</td>\n",
              "      <td>-0.709105</td>\n",
              "      <td>-0.673562</td>\n",
              "      <td>0.228081</td>\n",
              "      <td>-0.544217</td>\n",
              "      <td>-1.663180</td>\n",
              "      <td>0.546642</td>\n",
              "      <td>-0.089397</td>\n",
              "      <td>-0.886154</td>\n",
              "      <td>-0.807715</td>\n",
              "      <td>...</td>\n",
              "      <td>0.063068</td>\n",
              "      <td>-0.317946</td>\n",
              "      <td>-0.044252</td>\n",
              "      <td>-0.269611</td>\n",
              "      <td>-0.307904</td>\n",
              "      <td>-0.099221</td>\n",
              "      <td>0.456368</td>\n",
              "      <td>-0.229221</td>\n",
              "      <td>0.054851</td>\n",
              "      <td>-0.154021</td>\n",
              "      <td>-0.392028</td>\n",
              "      <td>0.052676</td>\n",
              "      <td>0.059684</td>\n",
              "      <td>-0.374149</td>\n",
              "      <td>-0.221023</td>\n",
              "      <td>-0.363490</td>\n",
              "      <td>0.343440</td>\n",
              "      <td>0.219747</td>\n",
              "      <td>0.096522</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>0.013602</td>\n",
              "      <td>-0.042015</td>\n",
              "      <td>0.169639</td>\n",
              "      <td>0.231231</td>\n",
              "      <td>-0.153053</td>\n",
              "      <td>-0.260412</td>\n",
              "      <td>0.524504</td>\n",
              "      <td>-0.517322</td>\n",
              "      <td>-0.857955</td>\n",
              "      <td>0.283597</td>\n",
              "      <td>0.031519</td>\n",
              "      <td>-0.182542</td>\n",
              "      <td>-0.209617</td>\n",
              "      <td>-0.358048</td>\n",
              "      <td>-0.080726</td>\n",
              "      <td>-0.123443</td>\n",
              "      <td>-0.123281</td>\n",
              "      <td>0.208734</td>\n",
              "      <td>-0.702139</td>\n",
              "      <td>1.419740</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 7130 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   1  -0.362772  -0.314085  -0.177185  ...  -0.009899  0.343084  -1.24348   1.80402\n",
              "0  1  -0.459580  -0.719855  -1.005840  ...  -0.573882 -0.729734 -0.639368  0.344724\n",
              "1  1   0.103909  -0.296076  -0.165474  ...   0.430106  0.181200 -0.626812  0.237308\n",
              "2  0  -0.110728  -0.147049  -0.402544  ...  -0.094244  0.731982 -1.464370  0.905178\n",
              "3  0   0.332963   1.002860   0.803334  ...   1.911490  1.815050 -0.559660 -0.699785\n",
              "4  0  -0.237573  -0.429426   0.841753  ...  -0.123281  0.208734 -0.702139  1.419740\n",
              "\n",
              "[5 rows x 7130 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRz8UPFD4t-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16cbf85c-1c01-4b2e-888c-041f21741508"
      },
      "source": [
        "db = np.loadtxt(\"/content/drive/My Drive/analis data/dataset/duke-breast-cancer.txt\")\n",
        "print(\"Database raw shape (%s,%s)\" % np.shape(db))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Database raw shape (86,7130)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Cwxw7P5rdQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5316d078-9f2a-4eae-b13f-016fb7ea3b4d"
      },
      "source": [
        "np.random.shuffle(db)\n",
        "y = db[:, 0]\n",
        "x = np.delete(db, [0], axis=1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n",
        "print(np.shape(x_train),np.shape(x_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(77, 7129) (9, 7129)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlwQ1ZQw6uYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_layer = np.zeros(72)\n",
        "weights = np.random.random((len(x[0]), 72))\n",
        "output_layer = np.zeros(2)\n",
        "hidden_weights = np.random.random((72, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhH5TNkP65_S",
        "colab_type": "text"
      },
      "source": [
        "Selanjutnya mengimplementasikan :\n",
        "- Sum function\n",
        "- Activation function\n",
        "- SoftMax function\n",
        "- Recalculate Weights function\n",
        "- Back-propagation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS9ZjLPe7LaI",
        "colab_type": "text"
      },
      "source": [
        "**Sum function**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "s_i is the sum for [i]th perceptron from the layer.\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUq5oBLx7ksa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sum_function(weights, index_locked_col, x):\n",
        "    result = 0\n",
        "    for i in range(0, len(x)):\n",
        "        result += x[i] * weights[i][index_locked_col]\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfaVbH_b7rxt",
        "colab_type": "text"
      },
      "source": [
        "**Activation function**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "g(s_i) is the activation for the [i]th perceptron from the layer.\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNyt9Kd373B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activate_layer(layer, weights, x):\n",
        "    for i in range(0, len(layer)):\n",
        "        layer[i] = 1.7159 * np.tanh(2.0 * sum_function(weights, i, x) / 3.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVT-oSfp8FFu",
        "colab_type": "text"
      },
      "source": [
        "**SoftMax function**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The softmax function, or normalized exponential function, is a generalization of the logistic function that \"squashes\" a K-dimensional vector z of arbitrary real values to a K-dimensional vector σ ( z ) of real values in the range (0, 1) that add up to 1.\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAUsAAABoCAYAAAB4+NKyAAAQbUlEQVR4Ae2d2bXqOBBFiYtIOgLiIYFO4351JgRD16DSgAbLg/DAuWu9h7GlUmnLPi4NNrc3/kAABEAABCYJ3CZTIAEIgAAIgMAbYomTAARAAAQ6CEAsOyAhCQiAAAhALHEOgAAIgEAHAYhlByQkAQEQAAGIJc4BEAABEOggALHsgIQkIAACIACxxDkAAiAAAh0EIJYdkJAEBEAABCCWOAdAAARAoIMAxLIDEpKAAAiAAMQS5wAIgAAIdBCAWHZAQhIQAAEQgFjiHAABEACBDgIQyw5ISAICIAACEEucAyAAAiDQQQBi2QEJSUAABEAAYolzAARAAAQ6CEAsOyAhCQiAAAhALHEOgAAIgEAHAYhlByQkAQEQAAGIJc4BEAABEOggALHsgIQkIAACIACxxDkAAiAAAh0EIJYdkJAEBH6GwOv5ftzv7+drXo1fz/v7/ni+/2bmm1fKvqkhlvvyR+kgcBgCInj353ux3pHQ3hcI7WEATDgCsZwAhMMg8BMEWOhuj/dftbKv99/zQWlu75v7d3/85cL696DjLTvVAg5/AGJ5+CaCgyAwmsDr/bzf3vdq31uP3+4kghZ2vv7eDxZNEsz0b8pWmvpM3yCWZ2ot+LoLgRdFSyGiur8fXjE2ckeiOo3Y2PYriuBUwCiqe9xdREflZ6JGedhHErw46mPvUt814ov3idZJ+fVxyr8H282jRe62V/ev6c5vhHVrMxDLrYnC3qUIqFAEIWGhud3C980qa4JM0dvTibGKEUV8PqJj0cyFS/aROJmGa77IR+ezDwKdOHvRl+O5GErdXNpS1FkTS3KyKKKbsdrJEMRyJ/Ao9gQE5KK/USQZ+eqFJtq3xWZJlErly75ICKnsF3WJTSjFlUI+L2wv7SbHdZJjlUgwE96oriXhlsNSl9THKNtpNyGWp206OD6agEVsNkwn5ZXEUsQpjcxUSEK32LrH8WcsWNT3lq5+EsEVRE+jtlyIXn+65CcMF3yIPE3F8Lgkl+8jSgewLpY2VlmaIa+NWZJRiOXoUxP2QeBIBMpi0Iq0Vnm/WCxNBElAebyTnSiJrOx2456k0vENYFIsE1V3tayUIUchlg4SPkDgJwioWCaRnkVnvstqQtWaSe6EtVQsXb5EzwpCxpNGMjHkjiWz2LIvjYzVa1e/xDgfaUWcdLhqT62e9X90w8/acvB7FQHrtvpuMU+sxOEWWU+74Ta58tkFVlHN9GSudyQw3IXmtYv2xwIn3ebIMYtsfTIvsi4fjV/67rblY/HyGbiXrBHm/ekizEYkaOWZKZped/ZJXD94Bb/Jvr+h2N7zf0Isz9+GqMFMArYMJxULGs/LLnASwmQ5Tphx9kU2hManmdxwXX632Ft0zYmgibnsk4jNxkGDaCdLjeiRw1eU9x+/5MjGMEM0zLZtaVJrnWVsn1cCFBej+zqq/TQi9wdPvQGxPHXzwfm5BEJUFYdFE93KViEsYJnItjIc9JgIcakrPtPfrezMLPYbySGW36CMMo5BwCKuT3Gz/VFXtddh6aovyNdr/5vp5EbyyWaOA8IxRLxzsp4hLcTyDK0EHzchkEWVtObQP+/MC79nlxJ1OUkobDH5bDMHymBjuX6MstM3Zou3DnXCQjIQODaBdKxOxgLpDTn3R3hiZon/XoApuow79ktsIc+xCSCyPHb7wLvNCJhYLokgN3MChk5MAGJ54saD63MINMSSx9rWjNXNcQNpT0sAYnnapoPjcwlYlzl+3E/fwHPdSYm5jJC+TgBiWWeDIxckkK8ZLKydvGC9UaX1BCCW6xn2W+AXpvKkAj05sdefPBUSvQZsLz9QLgicjQDE8kstpl3AAS+OXeC/Lg/RR+swg7sAILL8JIHjiSU/exo9ohWWeOTdpc/XYLXWBtt4lTzi1RzMz986faNoMB7nmnummFBOr13bvuy6r+4Ruxa0emYcAYGfI3AssaRHpexlAiHisVnM0iA8HZM3R9t7+irtZ09o8LO3LaGUbjLbisWRFy7biweCV5WS8t3y+Jc9l5sf9ntGlE3LrOV3Ugo/CSDlOi5XfI7Xc8UGCGxE4EBi6S7skpix4JT2EwSOLnlhsYhsMXRTseWf6AwvDijRM2GpiLK8UGHuGj0n9BXfgxcjyibrdpNoRI8anc+tV/AcWyDwKwSOI5YuAotfJTXdCCpGj6e+ZbqYV+zSUxqua1/TDevS16IsO17LX/S1M3Iz25uWXXSosNNxr5VdyIFdIPCTBI4nlpNRWNROIkYcCdaiUt3P4436jr9S1Ej2LAJrlD0laJFXfrNrrHJQ2d6JyQ3HbtZdYNIoEoDA5QgcRyz9+NqMWVoXNfJTuSKGH2InYiX7amKq7dkjhJamX1NMoNtdXLPbiuwsTXfZJsDyfsR2+f6t17Vxzcud8qgQCCwjcCCxpAqQ+PkfXPI//1mvmIihUxAVlEgYfNRJ+U08imrjhLT586YmfB0TNd7dtkBrslFlT9XZOykbyq4SdadJ3U8G2AtoOz8/bmKfJvEdBM5A4FhiycT8rLDOShfnbISsCphFZOkFnx7T7rC9FfqjWUxImxd0j6h92LVIuWV3WNnMsfBrgZ8uuu8pu0oi7AaBHydwPLGUBrHfOyHBrIqNCpgFi8n4oO+ea+s2xcBNcBQnh+zksDSZLy7izPZzxo7I0uxaJay8+NPSFMuIE6bbdoNombYcTT6WaPCnrKd1P6uA7c6IHbxkhQufL9/4+04pi2oy0fUVEYm63U5UHn+piE6NyZmoWIRaclXFpBSZalnlvNNiua7skqdhn/oc8QmHsq0jiGXmFHaAwMEIHFgsuSepayNL0ZEci6Mt69Ly3ba0P94XNYKVURY8TuhErzmmGRn0m1uI5biyvZtcQ1nYjzHLmAm2QeCTwP5iyRFhRchaF7Eci1XUi+XHRe8izpoYToll7bjtv1VnkS0yrkd3ZmOpb5+N6b8bi5iPP/i5Me3nZw58B4HFBOjc5JfJ1Ociypb5Wtn7Zyt2F0sVjJKgTI8HpiKjUVi6z6Kmxix2S1ic0DbFvCL03ORat8aJsaJsE9qSHraO5afidASc58EeEJhPgM/LVS9Z5utlgdDO97ScY3ex1OiRus501fsnr2lGvLWInF/YKo8ulpQirqctVqeu+aeIxsnMh/CyDHqhBf3AvSxjqoqhE/OWD+Zn4za6rGwT4vJNIBFLWV1Quhk5Ah0+xqywDQKLCEhg0DgP6er3Px7nJq6Kv08u52vLziLvujLtLpYsSvKORwdIZ0IbIbe7uG3GtK5VNt4XZhbrafOGuk2+81Ht120y/1Z0bO2zpGwTy0rUSifmQ55l5wX++duarGT+VLHe5+SL/cD2lQnodVAPWOw6ofPQIiYLdLILbMrWOI67i+W4qg22LKJdEau4aCfuWZvHaWZvl4ccZptxwwD1k3i2xeUZnC92E+z7PL/I689a2A09ftvVcpTFnBFf7kH5npPvdfFyPZ1QlbduJb2h/NWBHPXZX1oHbZN43+NfXvNbv1ZqN2ztIeVtLPurPT7zavtPiOVCprWGLJnTtPWTpZSntY/trRc4F3lHJ32rzG8csyEJvliTazUpnC9qE5f4QooudnchBUFgEUqMHOKL1iPUVYeXwvfNnaQbtw4thZ//1XOTeiD+iTnjG9iKn8TUor7i+eyCB8/ZibMMbcmxYC+pV+OGreUU8rXsJca3/QKxXMjTTiC659IbjQoN+mHXTrAwLvqR4Itf+U3p8ko7OrOt1/PF4htFRUMn/qorJbd0gbsM5/DV7C6kJw/vyNXtunhNe6UyBu8TPz/GnL3ADCq7JEwlP2RfEO0XdYlNKMWzUh464MXtpcwNueyvRIJ2XZRujnozCW3sqUg9gn9+/+ANiOVCwNrIFOH4O3KHIZls4ahwvzCHRWV6PLajLoOSeK7UPbSLrVRU7UKy/CHyPqZYhpttVLuGWEr6DyDKwKLs8meSZaFYsof6UyR07kZzC4ltqYZjLW0XbsPSJkWxdOmLxxo9H4hldNJg84cJhAuutmRL4Eh0k0cdmQiVBGJ3umUhUKEfGDGVWJSixCSytPYgvyi8FAks5XFM/bgnKanJ5aRY5qrregiVGybEcvczGA4chYC7GHmSJ0SIPc65Czu++JILv8fGN9KoWKZ1q0VZNuSwgYguEUuXJ0aqQx25kMlQCPenrf0sk3zPb2x+tYil8+hrLFyCqj1vYMgGuuFDsMLoWgKhizlDJAoXttopXahrPVyXP42AbVKlUtetxIHs2Fi1eS/DMtxtjgYNLcIVDfMC64aOaChJ10CneUQgI9FTG3yzowhTbJTrZmX54r19ajMLTc1Z91mPVD8SbvwVYrkxUJjbiIC7SGUJUXQRNq2LqKQXZRAlilZozWnl+muanXvQxvf88ides5sVTBGjWwsrEXRjPew24mARqo5txkJofso+YWjjn8qSBdXGKnndropfsGPCyHa0qVxkSN+lbs//RGDTSDpQje3zSojiYnSfXG3XbPlkAzYglgOgwuQ2BPKLsG1X06dRpLcxZyKuXUzzqI3ZSUTFKU30i5MYTVP+oAh+7w3D5zrYxobRcf19DGPrDLEcyxfWVxGIIpQVYrPKhRmZTZjTqMfVYbH/GhGeXSsZo/BZzEEMUISb9hxmNM/qpBDL1QhhYCgB3y1MI8ahZS4xXosgbf9StZP8+wnEEhStPDZEkQ9LtHKp0E49utu2sP4oxHI9Q1gYScCJZRqtjSxwme0sqqSF2f7FEDwEsMysX2S/OP/ScpEvIwCxzJBgx3EIuEmJhVFZmNwZXaNouMBNatzoVWJ3erKL1yau+fteHdZ4+Rt5IZa/0c4nrKUJ0NKozOVfKLQpMPfcedXWWl/T0vw3fuLLzzD7vdjYiQDEcifwKLZNwLq1VX1qZ9/oqOtK2xKfqjMNseQxx9mTGmZv30djN4J4GTMQy8s05YUqsnJSxIS2/faiDl70wpHel3FYmfGLUvQ1ZdeZnOkgdukkEMtLN+8ZK2dR1UT32wlqLdjbdqzP+VQrzGHOF1eH15qdsSXgc0oAYpnywLedCYjI9aylk1nyWtRWFje1bU+nlD/Leli2tzMqFP9lAhDLLwNHcQ0CvcuE3MRHvZutEyPbLTeCWDZa7WcOQSx/pqmPXlG3TMiW3nR9Vrrqmy/khlge/ez5hn8Qy29QRhmTBHq6yPbCh/BZFkudbMmP9ZSBbvhkU/1sAojlzzb9dSsuojh7uU6Dh3X7ySaepGlwuvghiOXFG/j3qrfuqZ+Ely1hyoYE8qg1yYcvlyQAsbxks/5wpSaWFP0wGVR9JQGI5UqAyH4kAjQRwz+Tu2UX/EjVgy+7EoBY7oofhW9FwCZv+NHCle+u2Mol2LkYAYjlxRoU1QEBEBhDAGI5hiusggAIXIwAxPJiDYrqgAAIjCEAsRzDFVZBAAQuRgBiebEGRXVAAATGEIBYjuEKqyAAAhcjALG8WIOiOiAAAmMIQCzHcIVVEACBixGAWF6sQVEdEACBMQQglmO4wioIgMDFCEAsL9agqA4IgMAYAhDLMVxhFQRA4GIEIJYXa1BUBwRAYAwBiOUYrrAKAiBwMQIQy4s1KKoDAiAwhgDEcgxXWAUBELgYgf8BqdpoEN6dSZEAAAAASUVORK5CYII=)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNaK9JF78WDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def soft_max(layer):\n",
        "    soft_max_output_layer = np.zeros(len(layer))\n",
        "    for i in range(0, len(layer)):\n",
        "        denominator = 0\n",
        "        for j in range(0, len(layer)):\n",
        "            denominator += np.exp(layer[j] - np.max(layer))\n",
        "        soft_max_output_layer[i] = np.exp(layer[i] - np.max(layer)) / denominator\n",
        "    return soft_max_output_layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh5DZ7-U8cqU",
        "colab_type": "text"
      },
      "source": [
        "**Recalculate weights function**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here we tune the network weights and hidden weights matrix. We are going to use this inside the back propagation function.\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_JrepjZ8jbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recalculate_weights(learning_rate, weights, gradient, activation):\n",
        "    for i in range(0, len(weights)):\n",
        "        for j in range(0, len(weights[i])):\n",
        "            weights[i][j] = (learning_rate * gradient[j] * activation[i]) + weights[i][j]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzPKLG-Z8pvj",
        "colab_type": "text"
      },
      "source": [
        "**Back-propagation functio**n\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this function we find out the output layer gradient and the hidden layer gradient to recalculate the network weights. Output gradient formula\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi-IqdUc8xJL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def back_propagation(hidden_layer, output_layer, one_hot_encoding, learning_rate, x):\n",
        "    output_derivative = np.zeros(2)\n",
        "    output_gradient = np.zeros(2)\n",
        "    for i in range(0, len(output_layer)):\n",
        "        output_derivative[i] = (1.0 - output_layer[i]) * output_layer[i]\n",
        "    for i in range(0, len(output_layer)):\n",
        "        output_gradient[i] = output_derivative[i] * (one_hot_encoding[i] - output_layer[i])\n",
        "    hidden_derivative = np.zeros(72)\n",
        "    hidden_gradient = np.zeros(72)\n",
        "    for i in range(0, len(hidden_layer)):\n",
        "        hidden_derivative[i] = (1.0 - hidden_layer[i]) * (1.0 + hidden_layer[i])\n",
        "    for i in range(0, len(hidden_layer)):\n",
        "        sum_ = 0\n",
        "        for j in range(0, len(output_gradient)):\n",
        "            sum_ += output_gradient[j] * hidden_weights[i][j]\n",
        "        hidden_gradient[i] = sum_ * hidden_derivative[i]\n",
        "    recalculate_weights(learning_rate, hidden_weights, output_gradient, hidden_layer)\n",
        "    recalculate_weights(learning_rate, weights, hidden_gradient, x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9K8IkVy9Rb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77bad9f0-9c82-42b4-da64-d6b6f0a8e29c"
      },
      "source": [
        "one_hot_encoding = np.zeros((2,2))\n",
        "for i in range(0, len(one_hot_encoding)):\n",
        "    one_hot_encoding[i][i] = 1\n",
        "training_correct_answers = 0\n",
        "for i in range(0, len(x_train)):\n",
        "    activate_layer(hidden_layer, weights, x_train[i])\n",
        "    activate_layer(output_layer, hidden_weights, hidden_layer)\n",
        "    output_layer = soft_max(output_layer)\n",
        "    training_correct_answers += 1 if y_train[i] == np.argmax(output_layer) else 0\n",
        "    back_propagation(hidden_layer, output_layer, one_hot_encoding[int(y_train[i])], -1, x_train[i])\n",
        "print(\"MLP Correct answers while learning: %s / %s (Accuracy = %s) on %s database.\" % (training_correct_answers, len(x_train), \n",
        "                                                                                       training_correct_answers/len(x_train),\"Duke breast cancer\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP Correct answers while learning: 58 / 77 (Accuracy = 0.7532467532467533) on Duke breast cancer database.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOMmKUMG9tOY",
        "colab_type": "text"
      },
      "source": [
        "Keakuratan tes tergantung pada matriks bobot yang dihasilkan secara acak dan tingkat pembelajaran. Menggunakan tingkat dan bobot pembelajaran yang berbeda akan menghasilkan akurasi yang berbeda.\n",
        "\n",
        "**pokok e tiap kali di running hasil e beda2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAwmGBZj9vYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cdec5d68-8cfb-4246-dca1-988f66bb5f2c"
      },
      "source": [
        "testing_correct_answers = 0\n",
        "for i in range(0, len(x_test)):\n",
        "    activate_layer(hidden_layer, weights, x_test[i])\n",
        "    activate_layer(output_layer, hidden_weights, hidden_layer)\n",
        "    output_layer = soft_max(output_layer)\n",
        "    testing_correct_answers += 1 if y_test[i] == np.argmax(output_layer) else 0\n",
        "print(\"MLP Correct answers while testing: %s / %s (Accuracy = %s) on %s database\" % (testing_correct_answers, len(x_test),\n",
        "                                                                                     testing_correct_answers/len(x_test), \"Duke breast cancer\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MLP Correct answers while testing: 5 / 9 (Accuracy = 0.5555555555555556) on Duke breast cancer database\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwkuVHgB-LX8",
        "colab_type": "text"
      },
      "source": [
        "Pada set pengujian ini, akurasi dapat mencapai bahkan 100% dengan jumlah tepat perceptron tersembunyi di lapisan tersembunyi. Dalam contoh ini, kami menggunakan tingkat pembelajaran [-1] dengan total [72] perceptron tersembunyi di lapisan tersembunyi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrJtvNk5-C2-",
        "colab_type": "text"
      },
      "source": [
        "**Kesimpulan**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Dalam tes ini, kami telah menunjukkan bahwa jaringan saraf back-propagation berkinerja baik pada set data yang besar. Kinerja dapat ditingkatkan dengan mengubah jumlah neuron tersembunyi dan tingkat pembelajaran. Karena pelatihan iteratif dan pelatihan berbasis gradien, kecepatan umum jauh lebih lambat dari yang dibutuhkan, sehingga dibutuhkan banyak waktu untuk berlatih pada kumpulan data yang sangat besar. Kami tidak dapat mengatakan bahwa ada jaringan yang sempurna untuk setiap jenis basis data di luar sana. Jadi, terus uji data Anda di beberapa jaringan saraf dan lihat apa yang paling cocok."
      ]
    }
  ]
}